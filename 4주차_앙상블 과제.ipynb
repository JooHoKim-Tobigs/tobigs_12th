{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 과제 :Dacon Mission 2: Predicting the opening/closing of hospitals!\n",
    "## Assignment  : \n",
    "- 지금까지 배운 아래 모델 모두를 활용하여 가장 좋은 방법을 찾아 accuracy 를 구해보고   \n",
    "  Test도 예측해서 Dacon2에 제출 해보세요! 가장 성능이 좋은 모델로 리더보드를 찍어보고 캡처해서 올려주세요.  \n",
    "  전처리는 제가 해서 드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data.pkl', 'rb') as f:\n",
    "    train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_data.pkl', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id=test['inst_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCLiabilities2</th>\n",
       "      <th>longLoan2</th>\n",
       "      <th>netAsset2</th>\n",
       "      <th>surplus2</th>\n",
       "      <th>employee1</th>\n",
       "      <th>employee2</th>\n",
       "      <th>ownerChange</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.361169e+08</td>\n",
       "      <td>3.900000e+08</td>\n",
       "      <td>2.619290e+09</td>\n",
       "      <td>1.271224e+09</td>\n",
       "      <td>62.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>801.0</td>\n",
       "      <td>813.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.073641e+10</td>\n",
       "      <td>1.510000e+10</td>\n",
       "      <td>1.295427e+10</td>\n",
       "      <td>7.740829e+09</td>\n",
       "      <td>663.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.136001e+10</td>\n",
       "      <td>1.410803e+10</td>\n",
       "      <td>5.561941e+06</td>\n",
       "      <td>9.025550e+09</td>\n",
       "      <td>206.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.000000e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.879004e+09</td>\n",
       "      <td>397.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.350000e+09</td>\n",
       "      <td>6.230000e+09</td>\n",
       "      <td>1.888829e+10</td>\n",
       "      <td>9.174283e+09</td>\n",
       "      <td>221.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.476517e+10</td>\n",
       "      <td>1.600000e+10</td>\n",
       "      <td>1.421786e+10</td>\n",
       "      <td>9.177283e+09</td>\n",
       "      <td>489.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.196268e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>243.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.523084e+10</td>\n",
       "      <td>3.400000e+09</td>\n",
       "      <td>2.024453e+10</td>\n",
       "      <td>9.554581e+09</td>\n",
       "      <td>370.0</td>\n",
       "      <td>382.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.384845e+10</td>\n",
       "      <td>1.944507e+10</td>\n",
       "      <td>1.305083e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>297.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.016196e+10</td>\n",
       "      <td>1.015280e+10</td>\n",
       "      <td>1.052376e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.858813e+10</td>\n",
       "      <td>2.516702e+10</td>\n",
       "      <td>9.699491e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>951.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.484903e+10</td>\n",
       "      <td>1.536245e+10</td>\n",
       "      <td>4.622382e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>951.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.620000e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.071706e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>183.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.154902e+10</td>\n",
       "      <td>1.822667e+10</td>\n",
       "      <td>5.876074e+10</td>\n",
       "      <td>5.248713e+08</td>\n",
       "      <td>409.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.440938e+08</td>\n",
       "      <td>42.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.268603e+09</td>\n",
       "      <td>1.943500e+08</td>\n",
       "      <td>3.385149e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>194.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.307440e+09</td>\n",
       "      <td>5.287440e+09</td>\n",
       "      <td>5.624237e+09</td>\n",
       "      <td>1.008090e+09</td>\n",
       "      <td>355.0</td>\n",
       "      <td>358.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.661981e+10</td>\n",
       "      <td>9.711298e+09</td>\n",
       "      <td>3.465059e+10</td>\n",
       "      <td>1.128104e+10</td>\n",
       "      <td>606.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.203208e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.032014e+09</td>\n",
       "      <td>6.019553e+09</td>\n",
       "      <td>117.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9.000000e+09</td>\n",
       "      <td>8.900000e+09</td>\n",
       "      <td>1.335174e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>164.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.042830e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.518461e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>266.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8.880445e+09</td>\n",
       "      <td>8.880445e+09</td>\n",
       "      <td>1.443617e+09</td>\n",
       "      <td>4.436172e+08</td>\n",
       "      <td>156.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9.240035e+09</td>\n",
       "      <td>2.620000e+09</td>\n",
       "      <td>-5.322708e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>243.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.190166e+09</td>\n",
       "      <td>3.533230e+09</td>\n",
       "      <td>5.596014e+09</td>\n",
       "      <td>4.069346e+09</td>\n",
       "      <td>148.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.944050e+10</td>\n",
       "      <td>8.374475e+09</td>\n",
       "      <td>8.102620e+09</td>\n",
       "      <td>2.692800e+04</td>\n",
       "      <td>418.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.050000e+10</td>\n",
       "      <td>9.300000e+09</td>\n",
       "      <td>7.374767e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>249.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1.710491e+09</td>\n",
       "      <td>1.609128e+09</td>\n",
       "      <td>1.357713e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>46.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>3.055704e+10</td>\n",
       "      <td>2.286030e+10</td>\n",
       "      <td>1.672659e+10</td>\n",
       "      <td>4.707295e+09</td>\n",
       "      <td>436.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>3.055704e+10</td>\n",
       "      <td>2.286030e+10</td>\n",
       "      <td>1.672659e+10</td>\n",
       "      <td>4.707295e+09</td>\n",
       "      <td>436.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1.593740e+09</td>\n",
       "      <td>1.336500e+09</td>\n",
       "      <td>1.472181e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>5.202417e+09</td>\n",
       "      <td>3.941668e+09</td>\n",
       "      <td>-4.125217e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>48.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>3.275500e+09</td>\n",
       "      <td>3.130000e+09</td>\n",
       "      <td>5.673234e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>385.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1.412320e+09</td>\n",
       "      <td>1.412320e+09</td>\n",
       "      <td>1.290230e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>2.960000e+09</td>\n",
       "      <td>2.860000e+09</td>\n",
       "      <td>1.481379e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>60.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>3.636189e+09</td>\n",
       "      <td>3.636189e+09</td>\n",
       "      <td>3.739328e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>51.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1.020000e+09</td>\n",
       "      <td>1.020000e+09</td>\n",
       "      <td>1.565328e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>2.224845e+09</td>\n",
       "      <td>2.224845e+09</td>\n",
       "      <td>3.772592e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>2.892490e+09</td>\n",
       "      <td>5.462400e+08</td>\n",
       "      <td>4.902827e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>53.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>63.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>6.356375e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.403602e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>101.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>4.299876e+09</td>\n",
       "      <td>2.722944e+09</td>\n",
       "      <td>5.357287e+09</td>\n",
       "      <td>1.873703e+09</td>\n",
       "      <td>180.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>4.016982e+10</td>\n",
       "      <td>1.216840e+09</td>\n",
       "      <td>3.309314e+10</td>\n",
       "      <td>4.915000e+08</td>\n",
       "      <td>807.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1.969747e+09</td>\n",
       "      <td>1.925500e+09</td>\n",
       "      <td>2.637783e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>6.090000e+09</td>\n",
       "      <td>5.890000e+09</td>\n",
       "      <td>9.065227e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>3.233945e+09</td>\n",
       "      <td>1.800000e+09</td>\n",
       "      <td>4.337819e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>62.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>6.805570e+09</td>\n",
       "      <td>5.930000e+09</td>\n",
       "      <td>5.379502e+09</td>\n",
       "      <td>8.116416e+08</td>\n",
       "      <td>193.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>79.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>4.199124e+09</td>\n",
       "      <td>2.498749e+09</td>\n",
       "      <td>3.259161e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>2.225928e+09</td>\n",
       "      <td>2.190000e+09</td>\n",
       "      <td>5.536178e+09</td>\n",
       "      <td>6.269440e+09</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     NCLiabilities2     longLoan2     netAsset2      surplus2  employee1  \\\n",
       "0      5.361169e+08  3.900000e+08  2.619290e+09  1.271224e+09       62.0   \n",
       "1     -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02      801.0   \n",
       "2      0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      234.0   \n",
       "3      2.073641e+10  1.510000e+10  1.295427e+10  7.740829e+09      663.0   \n",
       "4      2.136001e+10  1.410803e+10  5.561941e+06  9.025550e+09      206.0   \n",
       "5      2.000000e+07  0.000000e+00  0.000000e+00  5.879004e+09      397.0   \n",
       "6      6.350000e+09  6.230000e+09  1.888829e+10  9.174283e+09      221.0   \n",
       "7      2.476517e+10  1.600000e+10  1.421786e+10  9.177283e+09      489.0   \n",
       "8      4.196268e+07  0.000000e+00  0.000000e+00  0.000000e+00      243.0   \n",
       "9      1.523084e+10  3.400000e+09  2.024453e+10  9.554581e+09      370.0   \n",
       "10     2.384845e+10  1.944507e+10  1.305083e+10  0.000000e+00       53.0   \n",
       "11     0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      297.0   \n",
       "12     1.016196e+10  1.015280e+10  1.052376e+10  0.000000e+00       54.0   \n",
       "13     2.858813e+10  2.516702e+10  9.699491e+09  0.000000e+00      209.0   \n",
       "14    -9.990000e+02 -9.990000e+02 -9.990000e+02 -9.990000e+02      951.0   \n",
       "15     3.484903e+10  1.536245e+10  4.622382e+09  0.000000e+00      951.0   \n",
       "16     1.620000e+09  0.000000e+00  5.071706e+09  0.000000e+00      183.0   \n",
       "17     2.154902e+10  1.822667e+10  5.876074e+10  5.248713e+08      409.0   \n",
       "18     0.000000e+00  0.000000e+00  0.000000e+00  7.440938e+08       42.0   \n",
       "19     1.268603e+09  1.943500e+08  3.385149e+09  0.000000e+00      194.0   \n",
       "20     5.307440e+09  5.287440e+09  5.624237e+09  1.008090e+09      355.0   \n",
       "21     1.661981e+10  9.711298e+09  3.465059e+10  1.128104e+10      606.0   \n",
       "22     1.203208e+09  0.000000e+00  3.032014e+09  6.019553e+09      117.0   \n",
       "23     9.000000e+09  8.900000e+09  1.335174e+10  0.000000e+00      164.0   \n",
       "24     3.042830e+09  0.000000e+00  3.518461e+09  0.000000e+00      266.0   \n",
       "25     8.880445e+09  8.880445e+09  1.443617e+09  4.436172e+08      156.0   \n",
       "26     9.240035e+09  2.620000e+09 -5.322708e+08  0.000000e+00      243.0   \n",
       "27     4.190166e+09  3.533230e+09  5.596014e+09  4.069346e+09      148.0   \n",
       "28     1.944050e+10  8.374475e+09  8.102620e+09  2.692800e+04      418.0   \n",
       "29     1.050000e+10  9.300000e+09  7.374767e+09  0.000000e+00      249.0   \n",
       "..              ...           ...           ...           ...        ...   \n",
       "271    1.710491e+09  1.609128e+09  1.357713e+09  0.000000e+00       46.0   \n",
       "272    3.055704e+10  2.286030e+10  1.672659e+10  4.707295e+09      436.0   \n",
       "273    3.055704e+10  2.286030e+10  1.672659e+10  4.707295e+09      436.0   \n",
       "274    1.593740e+09  1.336500e+09  1.472181e+09  0.000000e+00       41.0   \n",
       "275    5.202417e+09  3.941668e+09 -4.125217e+09  0.000000e+00       48.0   \n",
       "276    3.275500e+09  3.130000e+09  5.673234e+09  0.000000e+00       84.0   \n",
       "277    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      385.0   \n",
       "278    1.412320e+09  1.412320e+09  1.290230e+09  0.000000e+00       75.0   \n",
       "279    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       48.0   \n",
       "280    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       95.0   \n",
       "281    2.960000e+09  2.860000e+09  1.481379e+09  0.000000e+00       60.0   \n",
       "282    3.636189e+09  3.636189e+09  3.739328e+09  0.000000e+00       51.0   \n",
       "283    1.020000e+09  1.020000e+09  1.565328e+09  0.000000e+00       17.0   \n",
       "284    2.224845e+09  2.224845e+09  3.772592e+08  0.000000e+00       36.0   \n",
       "285    2.892490e+09  5.462400e+08  4.902827e+09  0.000000e+00     -999.0   \n",
       "286    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       53.0   \n",
       "287    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       63.0   \n",
       "288    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       21.0   \n",
       "289    6.356375e+09  0.000000e+00  7.403602e+09  0.000000e+00      101.0   \n",
       "290    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      114.0   \n",
       "291    4.299876e+09  2.722944e+09  5.357287e+09  1.873703e+09      180.0   \n",
       "292    4.016982e+10  1.216840e+09  3.309314e+10  4.915000e+08      807.0   \n",
       "293    1.969747e+09  1.925500e+09  2.637783e+09  0.000000e+00       75.0   \n",
       "294    6.090000e+09  5.890000e+09  9.065227e+09  0.000000e+00      180.0   \n",
       "295    3.233945e+09  1.800000e+09  4.337819e+09  0.000000e+00       62.0   \n",
       "296    6.805570e+09  5.930000e+09  5.379502e+09  8.116416e+08      193.0   \n",
       "297    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00       79.0   \n",
       "298    4.199124e+09  2.498749e+09  3.259161e+09  0.000000e+00     -999.0   \n",
       "299    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00      100.0   \n",
       "300    2.225928e+09  2.190000e+09  5.536178e+09  6.269440e+09       15.0   \n",
       "\n",
       "     employee2  ownerChange  \n",
       "0         64.0            2  \n",
       "1        813.0            2  \n",
       "2          1.0            2  \n",
       "3        663.0            2  \n",
       "4        197.0            2  \n",
       "5        390.0            2  \n",
       "6        246.0            2  \n",
       "7        489.0            2  \n",
       "8        243.0            2  \n",
       "9        382.0            2  \n",
       "10        53.0            2  \n",
       "11       250.0            2  \n",
       "12        54.0            1  \n",
       "13       209.0            2  \n",
       "14       901.0            2  \n",
       "15       901.0            2  \n",
       "16       171.0            1  \n",
       "17       328.0            1  \n",
       "18        42.0            2  \n",
       "19       194.0            2  \n",
       "20       358.0            2  \n",
       "21       633.0            1  \n",
       "22       135.0            1  \n",
       "23       164.0            2  \n",
       "24       244.0            2  \n",
       "25       153.0            2  \n",
       "26       224.0            2  \n",
       "27       140.0            2  \n",
       "28       418.0            2  \n",
       "29       253.0            2  \n",
       "..         ...          ...  \n",
       "271       46.0            2  \n",
       "272      216.0            2  \n",
       "273      216.0            2  \n",
       "274       35.0            1  \n",
       "275       57.0            1  \n",
       "276       85.0            2  \n",
       "277      348.0            2  \n",
       "278       73.0            2  \n",
       "279        1.0            2  \n",
       "280       95.0            2  \n",
       "281       70.0            2  \n",
       "282       59.0            2  \n",
       "283       17.0            2  \n",
       "284       35.0            1  \n",
       "285     -999.0            0  \n",
       "286       49.0            1  \n",
       "287       63.0            1  \n",
       "288       23.0            2  \n",
       "289      104.0            1  \n",
       "290      114.0            1  \n",
       "291      178.0            2  \n",
       "292      583.0            2  \n",
       "293       70.0            2  \n",
       "294      180.0            2  \n",
       "295       66.0            2  \n",
       "296      141.0            2  \n",
       "297       79.0            2  \n",
       "298     -999.0            0  \n",
       "299     -999.0            0  \n",
       "300       15.0            2  \n",
       "\n",
       "[301 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:,1:11]#instkind 범주\n",
    "train.iloc[:,11:21]\n",
    "train.iloc[:,21:31] # receivableL1 분포가 이상함\n",
    "train.iloc[:,31:41]\n",
    "train.iloc[:,41:51]# recvivableL2 분포가 이상함\n",
    "train.iloc[:,51:] # ownerChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([train,test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sido']=df['sido'].astype(str)\n",
    "df['instkind']=df['instkind'].astype(str)\n",
    "df['ownerChange']=df['ownerChange'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3.010000e+02\n",
       "mean     2.449645e+05\n",
       "std      4.250438e+06\n",
       "min     -9.990000e+02\n",
       "25%      0.000000e+00\n",
       "50%      0.000000e+00\n",
       "75%      0.000000e+00\n",
       "max      7.374231e+07\n",
       "Name: receivableL1, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['receivableL1'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.query('receivableL1<0') # 대부분의 값이 -999 = 결측치로 생각됨  대부분의 변수가 -999라 대치도 어려워보임 -> 학습에 악영향 우려 ->삭제\n",
    "drop_id=train.query('receivableL1<0')['inst_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_sido=pd.get_dummies(df['sido'])\n",
    "dummy_sido.columns = ['sido' + '_' + str(column) for column in dummy_sido.columns]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_instkind=pd.get_dummies(df['instkind'])\n",
    "dummy_instkind.columns = ['instkind' + '_' + str(column) for column in dummy_instkind.columns]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_owner=pd.get_dummies(df['ownerChange'])\n",
    "dummy_owner.columns = ['ownerChagne' + '_' + str(column) for column in dummy_owner.columns]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['sido','instkind','ownerChange'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,dummy_sido,dummy_instkind,dummy_owner],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=df.query('inst_id in @test_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.query('inst_id not in @test_id & inst_id not in @drop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train=train.drop(['inst_id','OC'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train['OC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=test.drop(['inst_id','OC'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불균형 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 패키지 불러오기\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.combine import *\n",
    "from imblearn.under_sampling import *\n",
    "from imblearn.over_sampling import *\n",
    "from imblearn.ensemble import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Random oversampling of minority class\n",
    "ros = RandomOverSampler(random_state=7)\n",
    "# 3.2 SMOTE\n",
    "smote = SMOTE(random_state=7,n_jobs=-1)\n",
    "# 3.3 Borderline-SMOTE1\n",
    "bs1 = SMOTE(random_state=7,n_jobs=-1,kind='borderline1')\n",
    "# 3.4 Borderline-SMOTE2\n",
    "bs2 = SMOTE(random_state=7,n_jobs=-1,kind='borderline2')\n",
    "\n",
    "# 4.1 SMOTE + Tomek\n",
    "smtl = SMOTETomek(random_state=7)\n",
    "# 4.2 SMOTE + ENN\n",
    "smenn= SMOTEENN(random_state=7)\n",
    "\n",
    "models=[ros,smote,bs1,bs2,smtl,smenn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 2\n",
    "X_train2, X_test2, y_train2,y_test2 = train_test_split(X_train,y_train, test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imbal(model,X_train,y_train,X_test,y_test):\n",
    "    print(model)\n",
    "    print('\\n불균형 데이터 상태 %s' % Counter(y_train))\n",
    "    y_pred = LGBMClassifier(random_state=7,n_jobs=-1).fit(X_train, y_train).predict(X_test)\n",
    "    print('\\n불균형 데이터 모델 성능')\n",
    "    print('Accuracy = ', accuracy_score(y_test, y_pred)) \n",
    "    print('ROC_AUC = ',roc_auc_score(y_test, y_pred))\n",
    "    print('F1 = ',f1_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    X_res, y_res = model.fit_resample(X_train, y_train)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('Resampled 데이터 상태 %s' % Counter(y_res))\n",
    "    y_pred = LGBMClassifier(random_state=7,n_jobs=-1).fit(X_res, y_res).predict(X_test)\n",
    "    print('\\nResampled 데이터 모델 성능')\n",
    "    print('Accuracy = ', accuracy_score(y_test, y_pred)) \n",
    "    print('ROC_AUC = ',roc_auc_score(y_test, y_pred))\n",
    "    print('F1 = ',f1_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomOverSampler(random_state=7, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')\n",
      "\n",
      "불균형 데이터 상태 Counter({1: 195, 0: 10})\n",
      "\n",
      "불균형 데이터 모델 성능\n",
      "Accuracy =  0.9204545454545454\n",
      "ROC_AUC =  0.4879518072289157\n",
      "F1 =  0.9585798816568047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.98      0.96        83\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.92      0.90        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 2 81]]\n",
      "-----------------------------------------------------------------\n",
      "Resampled 데이터 상태 Counter({1: 195, 0: 195})\n",
      "\n",
      "Resampled 데이터 모델 성능\n",
      "Accuracy =  0.9431818181818182\n",
      "ROC_AUC =  0.5\n",
      "F1 =  0.9707602339181286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      1.00      0.97        83\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        88\n",
      "   macro avg       0.47      0.50      0.49        88\n",
      "weighted avg       0.89      0.94      0.92        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 0 83]]\n",
      "SMOTE(k_neighbors=5, kind='deprecated', m_neighbors='deprecated', n_jobs=-1,\n",
      "   out_step='deprecated', random_state=7, ratio=None,\n",
      "   sampling_strategy='auto', svm_estimator='deprecated')\n",
      "\n",
      "불균형 데이터 상태 Counter({1: 195, 0: 10})\n",
      "\n",
      "불균형 데이터 모델 성능\n",
      "Accuracy =  0.9204545454545454\n",
      "ROC_AUC =  0.4879518072289157\n",
      "F1 =  0.9585798816568047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.98      0.96        83\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.92      0.90        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 2 81]]\n",
      "-----------------------------------------------------------------\n",
      "Resampled 데이터 상태 Counter({1: 195, 0: 195})\n",
      "\n",
      "Resampled 데이터 모델 성능\n",
      "Accuracy =  0.9318181818181818\n",
      "ROC_AUC =  0.4939759036144578\n",
      "F1 =  0.9647058823529412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.99      0.96        83\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.93      0.91        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 1 82]]\n",
      "SMOTE(k_neighbors=5, kind='borderline1', m_neighbors='deprecated', n_jobs=-1,\n",
      "   out_step='deprecated', random_state=7, ratio=None,\n",
      "   sampling_strategy='auto', svm_estimator='deprecated')\n",
      "\n",
      "불균형 데이터 상태 Counter({1: 195, 0: 10})\n",
      "\n",
      "불균형 데이터 모델 성능\n",
      "Accuracy =  0.9204545454545454\n",
      "ROC_AUC =  0.4879518072289157\n",
      "F1 =  0.9585798816568047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.98      0.96        83\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.92      0.90        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 2 81]]\n",
      "-----------------------------------------------------------------\n",
      "Resampled 데이터 상태 Counter({1: 195, 0: 195})\n",
      "\n",
      "Resampled 데이터 모델 성능\n",
      "Accuracy =  0.9431818181818182\n",
      "ROC_AUC =  0.5\n",
      "F1 =  0.9707602339181286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      1.00      0.97        83\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        88\n",
      "   macro avg       0.47      0.50      0.49        88\n",
      "weighted avg       0.89      0.94      0.92        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 0 83]]\n",
      "SMOTE(k_neighbors=5, kind='borderline2', m_neighbors='deprecated', n_jobs=-1,\n",
      "   out_step='deprecated', random_state=7, ratio=None,\n",
      "   sampling_strategy='auto', svm_estimator='deprecated')\n",
      "\n",
      "불균형 데이터 상태 Counter({1: 195, 0: 10})\n",
      "\n",
      "불균형 데이터 모델 성능\n",
      "Accuracy =  0.9204545454545454\n",
      "ROC_AUC =  0.4879518072289157\n",
      "F1 =  0.9585798816568047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.98      0.96        83\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.92      0.90        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 2 81]]\n",
      "-----------------------------------------------------------------\n",
      "Resampled 데이터 상태 Counter({1: 195, 0: 195})\n",
      "\n",
      "Resampled 데이터 모델 성능\n",
      "Accuracy =  0.9545454545454546\n",
      "ROC_AUC =  0.6\n",
      "F1 =  0.976470588235294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.20      0.33         5\n",
      "           1       0.95      1.00      0.98        83\n",
      "\n",
      "   micro avg       0.95      0.95      0.95        88\n",
      "   macro avg       0.98      0.60      0.65        88\n",
      "weighted avg       0.96      0.95      0.94        88\n",
      "\n",
      "[[ 1  4]\n",
      " [ 0 83]]\n",
      "SMOTETomek(random_state=7, ratio=None, sampling_strategy='auto', smote=None,\n",
      "      tomek=None)\n",
      "\n",
      "불균형 데이터 상태 Counter({1: 195, 0: 10})\n",
      "\n",
      "불균형 데이터 모델 성능\n",
      "Accuracy =  0.9204545454545454\n",
      "ROC_AUC =  0.4879518072289157\n",
      "F1 =  0.9585798816568047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.98      0.96        83\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.92      0.90        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 2 81]]\n",
      "-----------------------------------------------------------------\n",
      "Resampled 데이터 상태 Counter({1: 193, 0: 193})\n",
      "\n",
      "Resampled 데이터 모델 성능\n",
      "Accuracy =  0.9318181818181818\n",
      "ROC_AUC =  0.4939759036144578\n",
      "F1 =  0.9647058823529412\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.99      0.96        83\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.93      0.91        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 1 82]]\n",
      "SMOTEENN(enn=None, random_state=7, ratio=None, sampling_strategy='auto',\n",
      "     smote=None)\n",
      "\n",
      "불균형 데이터 상태 Counter({1: 195, 0: 10})\n",
      "\n",
      "불균형 데이터 모델 성능\n",
      "Accuracy =  0.9204545454545454\n",
      "ROC_AUC =  0.4879518072289157\n",
      "F1 =  0.9585798816568047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      0.98      0.96        83\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        88\n",
      "   macro avg       0.47      0.49      0.48        88\n",
      "weighted avg       0.89      0.92      0.90        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 2 81]]\n",
      "-----------------------------------------------------------------\n",
      "Resampled 데이터 상태 Counter({0: 178, 1: 130})\n",
      "\n",
      "Resampled 데이터 모델 성능\n",
      "Accuracy =  0.9431818181818182\n",
      "ROC_AUC =  0.5\n",
      "F1 =  0.9707602339181286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.94      1.00      0.97        83\n",
      "\n",
      "   micro avg       0.94      0.94      0.94        88\n",
      "   macro avg       0.47      0.50      0.49        88\n",
      "weighted avg       0.89      0.94      0.92        88\n",
      "\n",
      "[[ 0  5]\n",
      " [ 0 83]]\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    imbal(model,X_train2,y_train2,X_test2,y_test2)\n",
    "# 모든 모델이 성능향상\n",
    "# 데이터셋 오버샘플링 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res=pd.DataFrame()\n",
    "y_res=pd.Series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    X,y =model.fit_resample(X_train,y_train)\n",
    "    X_res=pd.concat([X_res,pd.DataFrame(X)])\n",
    "    y_res=pd.concat([y_res,pd.Series(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res.columns=X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sgg</th>\n",
       "      <th>openDate</th>\n",
       "      <th>bedCount</th>\n",
       "      <th>revenue1</th>\n",
       "      <th>salescost1</th>\n",
       "      <th>sga1</th>\n",
       "      <th>salary1</th>\n",
       "      <th>noi1</th>\n",
       "      <th>noe1</th>\n",
       "      <th>interest1</th>\n",
       "      <th>...</th>\n",
       "      <th>instkind_1</th>\n",
       "      <th>instkind_2</th>\n",
       "      <th>instkind_3</th>\n",
       "      <th>instkind_4</th>\n",
       "      <th>instkind_5</th>\n",
       "      <th>instkind_6</th>\n",
       "      <th>instkind_7</th>\n",
       "      <th>ownerChagne_0</th>\n",
       "      <th>ownerChagne_1</th>\n",
       "      <th>ownerChagne_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.0</td>\n",
       "      <td>20071228.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>4.217530e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.961135e+09</td>\n",
       "      <td>2.033835e+09</td>\n",
       "      <td>1.565244e+07</td>\n",
       "      <td>1.523624e+07</td>\n",
       "      <td>1.323624e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.0</td>\n",
       "      <td>20161228.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>1.004522e+09</td>\n",
       "      <td>5.154837e+08</td>\n",
       "      <td>4.472197e+08</td>\n",
       "      <td>2.964023e+08</td>\n",
       "      <td>7.615600e+04</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141.0</td>\n",
       "      <td>20000814.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>7.250734e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.067740e+10</td>\n",
       "      <td>3.178605e+10</td>\n",
       "      <td>5.062231e+08</td>\n",
       "      <td>1.259568e+09</td>\n",
       "      <td>1.196881e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.0</td>\n",
       "      <td>20050901.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>4.904354e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.765605e+10</td>\n",
       "      <td>2.446078e+10</td>\n",
       "      <td>1.123523e+08</td>\n",
       "      <td>1.419089e+09</td>\n",
       "      <td>1.307249e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155.0</td>\n",
       "      <td>20020501.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>3.358054e+10</td>\n",
       "      <td>9.222997e+09</td>\n",
       "      <td>2.372791e+10</td>\n",
       "      <td>1.665533e+10</td>\n",
       "      <td>6.541432e+07</td>\n",
       "      <td>7.412694e+08</td>\n",
       "      <td>3.364622e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>169.0</td>\n",
       "      <td>19820702.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2.255218e+10</td>\n",
       "      <td>4.449958e+09</td>\n",
       "      <td>1.657333e+10</td>\n",
       "      <td>1.107396e+10</td>\n",
       "      <td>3.056460e+08</td>\n",
       "      <td>9.700088e+08</td>\n",
       "      <td>9.409428e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>66.0</td>\n",
       "      <td>19871102.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>6.435917e+10</td>\n",
       "      <td>2.075801e+10</td>\n",
       "      <td>4.088254e+10</td>\n",
       "      <td>2.682260e+10</td>\n",
       "      <td>1.426187e+09</td>\n",
       "      <td>3.166923e+09</td>\n",
       "      <td>4.659838e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>91.0</td>\n",
       "      <td>20060922.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>1.274839e+10</td>\n",
       "      <td>3.286977e+08</td>\n",
       "      <td>1.107619e+10</td>\n",
       "      <td>7.155937e+09</td>\n",
       "      <td>2.616211e+08</td>\n",
       "      <td>5.790795e+08</td>\n",
       "      <td>1.674719e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>20000124.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>4.140608e+10</td>\n",
       "      <td>6.624671e+09</td>\n",
       "      <td>3.512018e+10</td>\n",
       "      <td>1.979659e+10</td>\n",
       "      <td>7.084764e+08</td>\n",
       "      <td>1.000118e+09</td>\n",
       "      <td>4.485407e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>92.0</td>\n",
       "      <td>20080219.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>4.966917e+10</td>\n",
       "      <td>8.844037e+09</td>\n",
       "      <td>3.884561e+10</td>\n",
       "      <td>2.298501e+10</td>\n",
       "      <td>4.236134e+08</td>\n",
       "      <td>2.034358e+09</td>\n",
       "      <td>1.462730e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>136.0</td>\n",
       "      <td>20130605.0</td>\n",
       "      <td>656.0</td>\n",
       "      <td>1.481993e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.433499e+10</td>\n",
       "      <td>7.603419e+09</td>\n",
       "      <td>2.727323e+08</td>\n",
       "      <td>3.100628e+08</td>\n",
       "      <td>2.767684e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>108.0</td>\n",
       "      <td>20080909.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2.998496e+09</td>\n",
       "      <td>1.385685e+08</td>\n",
       "      <td>2.485503e+09</td>\n",
       "      <td>1.398272e+09</td>\n",
       "      <td>7.558900e+04</td>\n",
       "      <td>3.727438e+08</td>\n",
       "      <td>3.619707e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>140.0</td>\n",
       "      <td>20080924.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>6.697788e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.557784e+10</td>\n",
       "      <td>2.783384e+10</td>\n",
       "      <td>2.258753e+08</td>\n",
       "      <td>2.446764e+09</td>\n",
       "      <td>1.324954e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>127.0</td>\n",
       "      <td>19810622.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>1.240000e+11</td>\n",
       "      <td>3.574313e+10</td>\n",
       "      <td>9.132795e+10</td>\n",
       "      <td>5.778814e+10</td>\n",
       "      <td>1.314452e+09</td>\n",
       "      <td>1.570603e+09</td>\n",
       "      <td>8.969989e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.0</td>\n",
       "      <td>19820329.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>1.569070e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.569032e+10</td>\n",
       "      <td>8.142441e+09</td>\n",
       "      <td>5.112152e+08</td>\n",
       "      <td>2.474768e+09</td>\n",
       "      <td>3.197050e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>138.0</td>\n",
       "      <td>20121231.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>3.892860e+10</td>\n",
       "      <td>8.449315e+09</td>\n",
       "      <td>2.873623e+10</td>\n",
       "      <td>1.888037e+10</td>\n",
       "      <td>2.373243e+08</td>\n",
       "      <td>1.543530e+09</td>\n",
       "      <td>1.042223e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>49.0</td>\n",
       "      <td>20120103.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>6.857076e+09</td>\n",
       "      <td>3.161047e+08</td>\n",
       "      <td>6.011692e+09</td>\n",
       "      <td>2.849658e+09</td>\n",
       "      <td>2.623581e+07</td>\n",
       "      <td>5.138774e+08</td>\n",
       "      <td>8.859145e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>77.0</td>\n",
       "      <td>19940622.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2.292099e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.316153e+10</td>\n",
       "      <td>1.052172e+10</td>\n",
       "      <td>1.619736e+09</td>\n",
       "      <td>1.161622e+09</td>\n",
       "      <td>3.577082e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44.0</td>\n",
       "      <td>20050926.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1.709601e+10</td>\n",
       "      <td>1.148890e+09</td>\n",
       "      <td>1.551800e+10</td>\n",
       "      <td>8.231830e+09</td>\n",
       "      <td>2.250759e+07</td>\n",
       "      <td>4.745179e+08</td>\n",
       "      <td>4.720391e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>96.0</td>\n",
       "      <td>20061013.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>6.706899e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.349618e+10</td>\n",
       "      <td>2.979396e+10</td>\n",
       "      <td>2.592035e+09</td>\n",
       "      <td>5.335544e+09</td>\n",
       "      <td>2.841475e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>119.0</td>\n",
       "      <td>19820421.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>9.293956e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.098049e+10</td>\n",
       "      <td>5.578216e+09</td>\n",
       "      <td>1.252609e+08</td>\n",
       "      <td>2.351155e+08</td>\n",
       "      <td>2.311045e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>43.0</td>\n",
       "      <td>20080128.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>1.392766e+10</td>\n",
       "      <td>7.437995e+08</td>\n",
       "      <td>1.162402e+10</td>\n",
       "      <td>7.097651e+09</td>\n",
       "      <td>8.217133e+06</td>\n",
       "      <td>3.505226e+08</td>\n",
       "      <td>3.505226e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>114.0</td>\n",
       "      <td>19961210.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2.407472e+10</td>\n",
       "      <td>6.429352e+09</td>\n",
       "      <td>1.660325e+10</td>\n",
       "      <td>1.156730e+10</td>\n",
       "      <td>1.508636e+09</td>\n",
       "      <td>2.061558e+09</td>\n",
       "      <td>2.220963e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7.0</td>\n",
       "      <td>20040601.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>9.096548e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.732287e+09</td>\n",
       "      <td>4.287374e+09</td>\n",
       "      <td>3.371629e+07</td>\n",
       "      <td>2.860245e+08</td>\n",
       "      <td>2.685202e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40.0</td>\n",
       "      <td>19971231.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2.538852e+10</td>\n",
       "      <td>5.799018e+09</td>\n",
       "      <td>1.947479e+10</td>\n",
       "      <td>1.220574e+10</td>\n",
       "      <td>4.695500e+08</td>\n",
       "      <td>3.683014e+08</td>\n",
       "      <td>3.300710e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>172.0</td>\n",
       "      <td>20030318.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.480267e+10</td>\n",
       "      <td>1.462373e+09</td>\n",
       "      <td>1.294602e+10</td>\n",
       "      <td>8.418942e+09</td>\n",
       "      <td>7.589797e+07</td>\n",
       "      <td>3.503609e+08</td>\n",
       "      <td>3.225016e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>128.0</td>\n",
       "      <td>20010821.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>5.820145e+10</td>\n",
       "      <td>8.002571e+09</td>\n",
       "      <td>4.877858e+10</td>\n",
       "      <td>2.183969e+10</td>\n",
       "      <td>4.185674e+08</td>\n",
       "      <td>3.830671e+08</td>\n",
       "      <td>2.966072e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.0</td>\n",
       "      <td>20080225.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>1.673283e+10</td>\n",
       "      <td>2.568596e+09</td>\n",
       "      <td>1.235508e+10</td>\n",
       "      <td>7.040944e+09</td>\n",
       "      <td>4.011433e+07</td>\n",
       "      <td>3.752485e+08</td>\n",
       "      <td>3.438001e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>96.0</td>\n",
       "      <td>20010202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.320744e+10</td>\n",
       "      <td>2.301604e+09</td>\n",
       "      <td>1.026922e+10</td>\n",
       "      <td>5.671130e+09</td>\n",
       "      <td>1.505180e+07</td>\n",
       "      <td>3.143754e+08</td>\n",
       "      <td>1.727182e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>97.0</td>\n",
       "      <td>20080930.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>6.859739e+09</td>\n",
       "      <td>4.717129e+08</td>\n",
       "      <td>6.693224e+09</td>\n",
       "      <td>3.662477e+09</td>\n",
       "      <td>1.843976e+08</td>\n",
       "      <td>1.580904e+08</td>\n",
       "      <td>1.576394e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>79.0</td>\n",
       "      <td>19970728.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.065734e+10</td>\n",
       "      <td>3.411035e+10</td>\n",
       "      <td>5.127559e+10</td>\n",
       "      <td>1.837321e+10</td>\n",
       "      <td>2.491104e+08</td>\n",
       "      <td>6.505647e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>80.0</td>\n",
       "      <td>20040204.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.078418e+09</td>\n",
       "      <td>9.915723e+08</td>\n",
       "      <td>4.064324e+09</td>\n",
       "      <td>2.840441e+09</td>\n",
       "      <td>4.202070e+07</td>\n",
       "      <td>3.990376e+07</td>\n",
       "      <td>5.047630e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>80.0</td>\n",
       "      <td>19980328.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>1.410000e+11</td>\n",
       "      <td>4.220019e+10</td>\n",
       "      <td>1.030000e+11</td>\n",
       "      <td>6.403559e+10</td>\n",
       "      <td>2.425594e+09</td>\n",
       "      <td>2.517222e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>82.0</td>\n",
       "      <td>20040422.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.639142e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.152334e+09</td>\n",
       "      <td>2.434252e+09</td>\n",
       "      <td>2.086158e+07</td>\n",
       "      <td>3.011298e+08</td>\n",
       "      <td>9.714402e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>86.0</td>\n",
       "      <td>20100405.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>9.000000e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.070430e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>90.0</td>\n",
       "      <td>19830518.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>6.211947e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.196129e+10</td>\n",
       "      <td>3.103858e+10</td>\n",
       "      <td>2.351451e+09</td>\n",
       "      <td>2.076542e+08</td>\n",
       "      <td>1.945174e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>90.0</td>\n",
       "      <td>19820110.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.271967e+09</td>\n",
       "      <td>7.050922e+08</td>\n",
       "      <td>3.602864e+09</td>\n",
       "      <td>2.619079e+09</td>\n",
       "      <td>1.630604e+08</td>\n",
       "      <td>1.629970e+08</td>\n",
       "      <td>1.636140e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>92.0</td>\n",
       "      <td>20140214.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>2.619102e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.522124e+09</td>\n",
       "      <td>1.499650e+09</td>\n",
       "      <td>2.911800e+04</td>\n",
       "      <td>7.585388e+07</td>\n",
       "      <td>7.345388e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>95.0</td>\n",
       "      <td>20051101.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>8.834886e+09</td>\n",
       "      <td>2.524720e+08</td>\n",
       "      <td>6.669097e+09</td>\n",
       "      <td>4.367993e+09</td>\n",
       "      <td>7.781863e+07</td>\n",
       "      <td>1.912074e+09</td>\n",
       "      <td>7.087779e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>97.0</td>\n",
       "      <td>19940316.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>2.180513e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.224685e+10</td>\n",
       "      <td>1.214176e+10</td>\n",
       "      <td>2.690206e+08</td>\n",
       "      <td>1.588330e+08</td>\n",
       "      <td>1.069813e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>100.0</td>\n",
       "      <td>20020123.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.007769e+09</td>\n",
       "      <td>1.197756e+09</td>\n",
       "      <td>2.258240e+09</td>\n",
       "      <td>1.775322e+09</td>\n",
       "      <td>6.114894e+08</td>\n",
       "      <td>5.279657e+07</td>\n",
       "      <td>5.207657e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>104.0</td>\n",
       "      <td>20040916.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.244736e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.294404e+09</td>\n",
       "      <td>2.805394e+09</td>\n",
       "      <td>1.243661e+08</td>\n",
       "      <td>7.305216e+07</td>\n",
       "      <td>4.555502e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>109.0</td>\n",
       "      <td>19920801.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.323431e+10</td>\n",
       "      <td>1.078070e+10</td>\n",
       "      <td>3.318027e+10</td>\n",
       "      <td>2.052977e+10</td>\n",
       "      <td>3.541612e+09</td>\n",
       "      <td>7.728744e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>111.0</td>\n",
       "      <td>19880217.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2.746591e+09</td>\n",
       "      <td>4.698519e+08</td>\n",
       "      <td>3.343374e+09</td>\n",
       "      <td>1.332867e+09</td>\n",
       "      <td>4.971554e+08</td>\n",
       "      <td>4.402061e+08</td>\n",
       "      <td>3.439373e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>114.0</td>\n",
       "      <td>20111018.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>4.093586e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.366257e+09</td>\n",
       "      <td>2.799098e+09</td>\n",
       "      <td>4.856022e+07</td>\n",
       "      <td>1.466889e+08</td>\n",
       "      <td>1.426246e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>117.0</td>\n",
       "      <td>20111207.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>3.252112e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.121027e+09</td>\n",
       "      <td>1.716701e+09</td>\n",
       "      <td>6.040931e+07</td>\n",
       "      <td>1.066439e+08</td>\n",
       "      <td>1.066436e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>118.0</td>\n",
       "      <td>20041202.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.350015e+09</td>\n",
       "      <td>6.448866e+07</td>\n",
       "      <td>2.194768e+09</td>\n",
       "      <td>1.278552e+09</td>\n",
       "      <td>1.858200e+06</td>\n",
       "      <td>2.946142e+07</td>\n",
       "      <td>1.126125e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>125.0</td>\n",
       "      <td>20160826.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5.414029e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.580684e+10</td>\n",
       "      <td>2.270745e+10</td>\n",
       "      <td>2.469970e+08</td>\n",
       "      <td>1.756386e+09</td>\n",
       "      <td>7.074517e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>125.0</td>\n",
       "      <td>20140804.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>5.414029e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.580684e+10</td>\n",
       "      <td>2.270745e+10</td>\n",
       "      <td>2.469970e+08</td>\n",
       "      <td>1.756386e+09</td>\n",
       "      <td>7.074517e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>128.0</td>\n",
       "      <td>20150314.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2.132680e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.821933e+09</td>\n",
       "      <td>1.020400e+09</td>\n",
       "      <td>3.770175e+07</td>\n",
       "      <td>1.257182e+08</td>\n",
       "      <td>7.128413e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>132.0</td>\n",
       "      <td>20060825.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>2.360720e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.346809e+09</td>\n",
       "      <td>1.340152e+09</td>\n",
       "      <td>8.248638e+06</td>\n",
       "      <td>4.368790e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>136.0</td>\n",
       "      <td>20010307.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>3.724383e+10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.774321e+10</td>\n",
       "      <td>2.053163e+10</td>\n",
       "      <td>3.635211e+08</td>\n",
       "      <td>1.778202e+08</td>\n",
       "      <td>1.778202e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>137.0</td>\n",
       "      <td>20131021.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>4.527727e+09</td>\n",
       "      <td>2.292101e+08</td>\n",
       "      <td>4.062515e+09</td>\n",
       "      <td>2.317976e+09</td>\n",
       "      <td>4.243577e+06</td>\n",
       "      <td>9.786200e+07</td>\n",
       "      <td>7.201261e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>154.0</td>\n",
       "      <td>20090522.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>5.773933e+09</td>\n",
       "      <td>1.831330e+08</td>\n",
       "      <td>5.206654e+09</td>\n",
       "      <td>3.343153e+09</td>\n",
       "      <td>6.867192e+07</td>\n",
       "      <td>3.203096e+08</td>\n",
       "      <td>2.351390e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>154.0</td>\n",
       "      <td>19950515.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3.781154e+09</td>\n",
       "      <td>2.560143e+08</td>\n",
       "      <td>3.606557e+09</td>\n",
       "      <td>2.560967e+09</td>\n",
       "      <td>1.216177e+07</td>\n",
       "      <td>3.319011e+08</td>\n",
       "      <td>1.148727e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>160.0</td>\n",
       "      <td>20090703.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.087764e+09</td>\n",
       "      <td>4.321352e+08</td>\n",
       "      <td>2.110781e+09</td>\n",
       "      <td>1.390195e+09</td>\n",
       "      <td>5.468108e+08</td>\n",
       "      <td>7.500114e+07</td>\n",
       "      <td>7.470759e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>168.0</td>\n",
       "      <td>20080204.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>8.116130e+09</td>\n",
       "      <td>1.657443e+09</td>\n",
       "      <td>7.446937e+09</td>\n",
       "      <td>4.623183e+09</td>\n",
       "      <td>4.880531e+07</td>\n",
       "      <td>2.940289e+08</td>\n",
       "      <td>2.491998e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>169.0</td>\n",
       "      <td>20030120.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.910482e+10</td>\n",
       "      <td>9.454929e+08</td>\n",
       "      <td>1.867590e+10</td>\n",
       "      <td>1.001050e+10</td>\n",
       "      <td>3.441777e+08</td>\n",
       "      <td>3.280312e+08</td>\n",
       "      <td>2.555981e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>169.0</td>\n",
       "      <td>20020401.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>1.000000e+11</td>\n",
       "      <td>2.647023e+10</td>\n",
       "      <td>6.427413e+10</td>\n",
       "      <td>4.302536e+10</td>\n",
       "      <td>6.959476e+09</td>\n",
       "      <td>8.294444e+09</td>\n",
       "      <td>1.640537e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>178.0</td>\n",
       "      <td>20050211.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>7.614697e+09</td>\n",
       "      <td>4.153475e+08</td>\n",
       "      <td>5.903119e+09</td>\n",
       "      <td>3.467861e+09</td>\n",
       "      <td>4.209897e+07</td>\n",
       "      <td>7.439421e+08</td>\n",
       "      <td>1.875761e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3191 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sgg    openDate  bedCount      revenue1    salescost1          sga1  \\\n",
       "0     73.0  20071228.0     175.0  4.217530e+09  0.000000e+00  3.961135e+09   \n",
       "1     89.0  20161228.0     468.0  1.004522e+09  5.154837e+08  4.472197e+08   \n",
       "2    141.0  20000814.0     353.0  7.250734e+10  0.000000e+00  7.067740e+10   \n",
       "3     32.0  20050901.0     196.0  4.904354e+10  0.000000e+00  4.765605e+10   \n",
       "4    155.0  20020501.0     243.0  3.358054e+10  9.222997e+09  2.372791e+10   \n",
       "5    169.0  19820702.0     213.0  2.255218e+10  4.449958e+09  1.657333e+10   \n",
       "6     66.0  19871102.0     250.0  6.435917e+10  2.075801e+10  4.088254e+10   \n",
       "7     91.0  20060922.0     280.0  1.274839e+10  3.286977e+08  1.107619e+10   \n",
       "8      4.0  20000124.0      55.0  4.140608e+10  6.624671e+09  3.512018e+10   \n",
       "9     92.0  20080219.0     250.0  4.966917e+10  8.844037e+09  3.884561e+10   \n",
       "10   136.0  20130605.0     656.0  1.481993e+10  0.000000e+00  1.433499e+10   \n",
       "11   108.0  20080909.0     123.0  2.998496e+09  1.385685e+08  2.485503e+09   \n",
       "12   140.0  20080924.0     143.0  6.697788e+10  0.000000e+00  6.557784e+10   \n",
       "13   127.0  19810622.0     526.0  1.240000e+11  3.574313e+10  9.132795e+10   \n",
       "14     3.0  19820329.0     181.0  1.569070e+10  0.000000e+00  1.569032e+10   \n",
       "15   138.0  20121231.0     243.0  3.892860e+10  8.449315e+09  2.873623e+10   \n",
       "16    49.0  20120103.0     291.0  6.857076e+09  3.161047e+08  6.011692e+09   \n",
       "17    77.0  19940622.0     195.0  2.292099e+10  0.000000e+00  2.316153e+10   \n",
       "18    44.0  20050926.0     167.0  1.709601e+10  1.148890e+09  1.551800e+10   \n",
       "19    96.0  20061013.0     346.0  6.706899e+10  0.000000e+00  6.349618e+10   \n",
       "20   119.0  19820421.0      48.0  9.293956e+09  0.000000e+00  1.098049e+10   \n",
       "21    43.0  20080128.0     252.0  1.392766e+10  7.437995e+08  1.162402e+10   \n",
       "22   114.0  19961210.0      66.0  2.407472e+10  6.429352e+09  1.660325e+10   \n",
       "23     7.0  20040601.0     196.0  9.096548e+09  0.000000e+00  8.732287e+09   \n",
       "24    40.0  19971231.0     137.0  2.538852e+10  5.799018e+09  1.947479e+10   \n",
       "25   172.0  20030318.0      63.0  1.480267e+10  1.462373e+09  1.294602e+10   \n",
       "26   128.0  20010821.0     149.0  5.820145e+10  8.002571e+09  4.877858e+10   \n",
       "27    23.0  20080225.0     406.0  1.673283e+10  2.568596e+09  1.235508e+10   \n",
       "28    96.0  20010202.0       0.0  1.320744e+10  2.301604e+09  1.026922e+10   \n",
       "29    97.0  20080930.0      78.0  6.859739e+09  4.717129e+08  6.693224e+09   \n",
       "..     ...         ...       ...           ...           ...           ...   \n",
       "393   79.0  19970728.0       0.0  9.065734e+10  3.411035e+10  5.127559e+10   \n",
       "394   80.0  20040204.0      22.0  5.078418e+09  9.915723e+08  4.064324e+09   \n",
       "395   80.0  19980328.0     523.0  1.410000e+11  4.220019e+10  1.030000e+11   \n",
       "396   82.0  20040422.0      14.0  4.639142e+09  0.000000e+00  4.152334e+09   \n",
       "397   86.0  20100405.0     193.0  9.000000e+06  0.000000e+00  4.070430e+06   \n",
       "398   90.0  19830518.0     339.0  6.211947e+10  0.000000e+00  6.196129e+10   \n",
       "399   90.0  19820110.0      24.0  4.271967e+09  7.050922e+08  3.602864e+09   \n",
       "400   92.0  20140214.0     137.0  2.619102e+09  0.000000e+00  2.522124e+09   \n",
       "401   95.0  20051101.0     154.0  8.834886e+09  2.524720e+08  6.669097e+09   \n",
       "402   97.0  19940316.0     247.0  2.180513e+10  0.000000e+00  2.224685e+10   \n",
       "403  100.0  20020123.0      39.0  3.007769e+09  1.197756e+09  2.258240e+09   \n",
       "404  104.0  20040916.0     136.0  5.244736e+09  0.000000e+00  5.294404e+09   \n",
       "405  109.0  19920801.0      52.0  5.323431e+10  1.078070e+10  3.318027e+10   \n",
       "406  111.0  19880217.0      45.0  2.746591e+09  4.698519e+08  3.343374e+09   \n",
       "407  114.0  20111018.0     145.0  4.093586e+09  0.000000e+00  4.366257e+09   \n",
       "408  117.0  20111207.0     152.0  3.252112e+09  0.000000e+00  3.121027e+09   \n",
       "409  118.0  20041202.0      88.0  2.350015e+09  6.448866e+07  2.194768e+09   \n",
       "410  125.0  20160826.0     176.0  5.414029e+10  0.000000e+00  4.580684e+10   \n",
       "411  125.0  20140804.0     235.0  5.414029e+10  0.000000e+00  4.580684e+10   \n",
       "412  128.0  20150314.0      42.0  2.132680e+09  0.000000e+00  1.821933e+09   \n",
       "413  132.0  20060825.0     113.0  2.360720e+09  0.000000e+00  2.346809e+09   \n",
       "414  136.0  20010307.0     248.0  3.724383e+10  0.000000e+00  3.774321e+10   \n",
       "415  137.0  20131021.0     171.0  4.527727e+09  2.292101e+08  4.062515e+09   \n",
       "416  154.0  20090522.0     199.0  5.773933e+09  1.831330e+08  5.206654e+09   \n",
       "417  154.0  19950515.0      66.0  3.781154e+09  2.560143e+08  3.606557e+09   \n",
       "418  160.0  20090703.0      90.0  2.087764e+09  4.321352e+08  2.110781e+09   \n",
       "419  168.0  20080204.0      86.0  8.116130e+09  1.657443e+09  7.446937e+09   \n",
       "420  169.0  20030120.0     151.0  1.910482e+10  9.454929e+08  1.867590e+10   \n",
       "421  169.0  20020401.0     306.0  1.000000e+11  2.647023e+10  6.427413e+10   \n",
       "422  178.0  20050211.0     214.0  7.614697e+09  4.153475e+08  5.903119e+09   \n",
       "\n",
       "          salary1          noi1          noe1     interest1  ...  instkind_1  \\\n",
       "0    2.033835e+09  1.565244e+07  1.523624e+07  1.323624e+07  ...         0.0   \n",
       "1    2.964023e+08  7.615600e+04  3.000000e+04  0.000000e+00  ...         0.0   \n",
       "2    3.178605e+10  5.062231e+08  1.259568e+09  1.196881e+09  ...         0.0   \n",
       "3    2.446078e+10  1.123523e+08  1.419089e+09  1.307249e+09  ...         0.0   \n",
       "4    1.665533e+10  6.541432e+07  7.412694e+08  3.364622e+08  ...         0.0   \n",
       "5    1.107396e+10  3.056460e+08  9.700088e+08  9.409428e+08  ...         0.0   \n",
       "6    2.682260e+10  1.426187e+09  3.166923e+09  4.659838e+08  ...         0.0   \n",
       "7    7.155937e+09  2.616211e+08  5.790795e+08  1.674719e+08  ...         0.0   \n",
       "8    1.979659e+10  7.084764e+08  1.000118e+09  4.485407e+08  ...         0.0   \n",
       "9    2.298501e+10  4.236134e+08  2.034358e+09  1.462730e+09  ...         0.0   \n",
       "10   7.603419e+09  2.727323e+08  3.100628e+08  2.767684e+08  ...         0.0   \n",
       "11   1.398272e+09  7.558900e+04  3.727438e+08  3.619707e+08  ...         0.0   \n",
       "12   2.783384e+10  2.258753e+08  2.446764e+09  1.324954e+09  ...         0.0   \n",
       "13   5.778814e+10  1.314452e+09  1.570603e+09  8.969989e+08  ...         0.0   \n",
       "14   8.142441e+09  5.112152e+08  2.474768e+09  3.197050e+08  ...         0.0   \n",
       "15   1.888037e+10  2.373243e+08  1.543530e+09  1.042223e+09  ...         0.0   \n",
       "16   2.849658e+09  2.623581e+07  5.138774e+08  8.859145e+07  ...         0.0   \n",
       "17   1.052172e+10  1.619736e+09  1.161622e+09  3.577082e+08  ...         0.0   \n",
       "18   8.231830e+09  2.250759e+07  4.745179e+08  4.720391e+08  ...         0.0   \n",
       "19   2.979396e+10  2.592035e+09  5.335544e+09  2.841475e+09  ...         0.0   \n",
       "20   5.578216e+09  1.252609e+08  2.351155e+08  2.311045e+08  ...         0.0   \n",
       "21   7.097651e+09  8.217133e+06  3.505226e+08  3.505226e+08  ...         0.0   \n",
       "22   1.156730e+10  1.508636e+09  2.061558e+09  2.220963e+08  ...         0.0   \n",
       "23   4.287374e+09  3.371629e+07  2.860245e+08  2.685202e+08  ...         0.0   \n",
       "24   1.220574e+10  4.695500e+08  3.683014e+08  3.300710e+08  ...         0.0   \n",
       "25   8.418942e+09  7.589797e+07  3.503609e+08  3.225016e+08  ...         0.0   \n",
       "26   2.183969e+10  4.185674e+08  3.830671e+08  2.966072e+08  ...         0.0   \n",
       "27   7.040944e+09  4.011433e+07  3.752485e+08  3.438001e+08  ...         0.0   \n",
       "28   5.671130e+09  1.505180e+07  3.143754e+08  1.727182e+08  ...         0.0   \n",
       "29   3.662477e+09  1.843976e+08  1.580904e+08  1.576394e+08  ...         0.0   \n",
       "..            ...           ...           ...           ...  ...         ...   \n",
       "393  1.837321e+10  2.491104e+08  6.505647e+08  0.000000e+00  ...         1.0   \n",
       "394  2.840441e+09  4.202070e+07  3.990376e+07  5.047630e+05  ...         0.0   \n",
       "395  6.403559e+10  2.425594e+09  2.517222e+09  0.000000e+00  ...         0.0   \n",
       "396  2.434252e+09  2.086158e+07  3.011298e+08  9.714402e+07  ...         1.0   \n",
       "397  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  ...         0.0   \n",
       "398  3.103858e+10  2.351451e+09  2.076542e+08  1.945174e+07  ...         0.0   \n",
       "399  2.619079e+09  1.630604e+08  1.629970e+08  1.636140e+05  ...         0.0   \n",
       "400  1.499650e+09  2.911800e+04  7.585388e+07  7.345388e+07  ...         0.0   \n",
       "401  4.367993e+09  7.781863e+07  1.912074e+09  7.087779e+07  ...         0.0   \n",
       "402  1.214176e+10  2.690206e+08  1.588330e+08  1.069813e+08  ...         0.0   \n",
       "403  1.775322e+09  6.114894e+08  5.279657e+07  5.207657e+07  ...         0.0   \n",
       "404  2.805394e+09  1.243661e+08  7.305216e+07  4.555502e+06  ...         0.0   \n",
       "405  2.052977e+10  3.541612e+09  7.728744e+09  0.000000e+00  ...         0.0   \n",
       "406  1.332867e+09  4.971554e+08  4.402061e+08  3.439373e+08  ...         0.0   \n",
       "407  2.799098e+09  4.856022e+07  1.466889e+08  1.426246e+08  ...         0.0   \n",
       "408  1.716701e+09  6.040931e+07  1.066439e+08  1.066436e+08  ...         0.0   \n",
       "409  1.278552e+09  1.858200e+06  2.946142e+07  1.126125e+07  ...         0.0   \n",
       "410  2.270745e+10  2.469970e+08  1.756386e+09  7.074517e+08  ...         0.0   \n",
       "411  2.270745e+10  2.469970e+08  1.756386e+09  7.074517e+08  ...         0.0   \n",
       "412  1.020400e+09  3.770175e+07  1.257182e+08  7.128413e+07  ...         0.0   \n",
       "413  1.340152e+09  8.248638e+06  4.368790e+06  0.000000e+00  ...         0.0   \n",
       "414  2.053163e+10  3.635211e+08  1.778202e+08  1.778202e+08  ...         0.0   \n",
       "415  2.317976e+09  4.243577e+06  9.786200e+07  7.201261e+07  ...         0.0   \n",
       "416  3.343153e+09  6.867192e+07  3.203096e+08  2.351390e+08  ...         0.0   \n",
       "417  2.560967e+09  1.216177e+07  3.319011e+08  1.148727e+08  ...         0.0   \n",
       "418  1.390195e+09  5.468108e+08  7.500114e+07  7.470759e+07  ...         0.0   \n",
       "419  4.623183e+09  4.880531e+07  2.940289e+08  2.491998e+08  ...         0.0   \n",
       "420  1.001050e+10  3.441777e+08  3.280312e+08  2.555981e+08  ...         0.0   \n",
       "421  4.302536e+10  6.959476e+09  8.294444e+09  1.640537e+08  ...         0.0   \n",
       "422  3.467861e+09  4.209897e+07  7.439421e+08  1.875761e+08  ...         0.0   \n",
       "\n",
       "     instkind_2  instkind_3  instkind_4  instkind_5  instkind_6  instkind_7  \\\n",
       "0           0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "1           0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "2           0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "3           0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "4           0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "5           0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "6           0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "7           0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "8           0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "9           0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "10          0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "11          0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "12          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "13          0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "14          0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "15          0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "16          0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "17          0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "18          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "19          0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "20          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "21          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "22          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "23          0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "24          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "25          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "26          0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "27          0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "28          0.0         0.0         0.0         0.0         1.0         0.0   \n",
       "29          0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "393         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "394         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "395         0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "396         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "397         0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "398         0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "399         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "400         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "401         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "402         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "403         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "404         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "405         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "406         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "407         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "408         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "409         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "410         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "411         0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "412         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "413         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "414         0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "415         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "416         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "417         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "418         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "419         0.0         0.0         1.0         0.0         0.0         0.0   \n",
       "420         0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "421         0.0         1.0         0.0         0.0         0.0         0.0   \n",
       "422         0.0         0.0         0.0         1.0         0.0         0.0   \n",
       "\n",
       "     ownerChagne_0  ownerChagne_1  ownerChagne_2  \n",
       "0              0.0            0.0            1.0  \n",
       "1              0.0            0.0            1.0  \n",
       "2              0.0            0.0            1.0  \n",
       "3              0.0            0.0            1.0  \n",
       "4              0.0            0.0            1.0  \n",
       "5              0.0            0.0            1.0  \n",
       "6              0.0            0.0            1.0  \n",
       "7              0.0            0.0            1.0  \n",
       "8              0.0            0.0            1.0  \n",
       "9              0.0            0.0            1.0  \n",
       "10             0.0            0.0            1.0  \n",
       "11             0.0            1.0            0.0  \n",
       "12             0.0            0.0            1.0  \n",
       "13             0.0            0.0            1.0  \n",
       "14             0.0            1.0            0.0  \n",
       "15             0.0            1.0            0.0  \n",
       "16             0.0            0.0            1.0  \n",
       "17             0.0            0.0            1.0  \n",
       "18             0.0            0.0            1.0  \n",
       "19             0.0            1.0            0.0  \n",
       "20             0.0            1.0            0.0  \n",
       "21             0.0            0.0            1.0  \n",
       "22             0.0            0.0            1.0  \n",
       "23             0.0            0.0            1.0  \n",
       "24             0.0            0.0            1.0  \n",
       "25             0.0            0.0            1.0  \n",
       "26             0.0            0.0            1.0  \n",
       "27             0.0            0.0            1.0  \n",
       "28             0.0            0.0            1.0  \n",
       "29             0.0            0.0            1.0  \n",
       "..             ...            ...            ...  \n",
       "393            0.0            0.0            1.0  \n",
       "394            0.0            0.0            1.0  \n",
       "395            0.0            1.0            0.0  \n",
       "396            0.0            0.0            1.0  \n",
       "397            0.0            1.0            0.0  \n",
       "398            0.0            0.0            1.0  \n",
       "399            0.0            0.0            1.0  \n",
       "400            0.0            0.0            1.0  \n",
       "401            0.0            0.0            1.0  \n",
       "402            0.0            0.0            1.0  \n",
       "403            1.0            0.0            0.0  \n",
       "404            0.0            0.0            1.0  \n",
       "405            0.0            0.0            1.0  \n",
       "406            0.0            1.0            0.0  \n",
       "407            0.0            1.0            0.0  \n",
       "408            0.0            0.0            1.0  \n",
       "409            0.0            0.0            1.0  \n",
       "410            0.0            0.0            1.0  \n",
       "411            0.0            0.0            1.0  \n",
       "412            0.0            1.0            0.0  \n",
       "413            0.0            1.0            0.0  \n",
       "414            0.0            0.0            1.0  \n",
       "415            0.0            0.0            1.0  \n",
       "416            0.0            0.0            1.0  \n",
       "417            0.0            0.0            1.0  \n",
       "418            0.0            1.0            0.0  \n",
       "419            0.0            1.0            0.0  \n",
       "420            0.0            0.0            1.0  \n",
       "421            0.0            0.0            1.0  \n",
       "422            0.0            0.0            1.0  \n",
       "\n",
       "[3191 rows x 81 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB=LGBMClassifier()\n",
    "kfold=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.87735849, 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LGB,X_res,y_res,scoring='accuracy',cv=kfold,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "       0.93103448, 0.96551724, 0.93103448, 0.96428571, 0.96428571])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LGB,X_train,y_train,scoring='accuracy',cv=kfold,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2주차)Linear Regression\n",
    "## 3주차) KNN, Naive Bayesian\n",
    "## 4주차) DT(단일 모델), Random Forest, AdaBoost, XGBoost, LightGBM, STACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn 패키지 불러오기\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설정\n",
    "random_state = 2\n",
    "classifiers = []\n",
    "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state = random_state))\n",
    "classifiers.append(XGBClassifier(random_state = random_state))\n",
    "classifiers.append(LGBMClassifier(random_state=random_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_score_check(score,X_train,y_train):\n",
    "    cv_results = []\n",
    "    for classifier in classifiers :\n",
    "        cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring=str(score),cv = kfold, n_jobs=-1))\n",
    "        cv_means = []\n",
    "    cv_std = []\n",
    "    for cv_result in cv_results:\n",
    "        cv_means.append(cv_result.mean())\n",
    "        cv_std.append(cv_result.std())\n",
    "    cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"DecisionTree\",\"AdaBoost\",\n",
    "                                                                                          \"RandomForest\",\"GradientBoosting\",\"KNeighboors\",\"LogisticRegression\",'XGBoost','LGBM']})\n",
    "    cv_res.sort_values(by='CrossValMeans',ascending=False,inplace=True)\n",
    "    g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "    g.set_xlabel(\"Mean \"+str(score))\n",
    "    g = g.set_title(\"Cross validation scores\")\n",
    "    display(cv_res)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrossValMeans</th>\n",
       "      <th>CrossValerrors</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.949179</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>KNeighboors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.942282</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.942282</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.942159</td>\n",
       "      <td>0.021215</td>\n",
       "      <td>GradientBoosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.938711</td>\n",
       "      <td>0.013183</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.907882</td>\n",
       "      <td>0.055234</td>\n",
       "      <td>AdaBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.894195</td>\n",
       "      <td>0.052852</td>\n",
       "      <td>DecisionTree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.863859</td>\n",
       "      <td>0.062944</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrossValMeans  CrossValerrors           Algorithm\n",
       "4       0.949179        0.015851         KNeighboors\n",
       "2       0.942282        0.014702        RandomForest\n",
       "7       0.942282        0.014702                LGBM\n",
       "3       0.942159        0.021215    GradientBoosting\n",
       "6       0.938711        0.013183             XGBoost\n",
       "1       0.907882        0.055234            AdaBoost\n",
       "0       0.894195        0.052852        DecisionTree\n",
       "5       0.863859        0.062944  LogisticRegression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_score_check('accuracy',X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrossValMeans</th>\n",
       "      <th>CrossValerrors</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.989623</td>\n",
       "      <td>0.031132</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.989623</td>\n",
       "      <td>0.031132</td>\n",
       "      <td>GradientBoosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.988375</td>\n",
       "      <td>0.029737</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>DecisionTree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.987736</td>\n",
       "      <td>0.036792</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.986164</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>AdaBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.983025</td>\n",
       "      <td>0.045771</td>\n",
       "      <td>KNeighboors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.886558</td>\n",
       "      <td>0.040542</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrossValMeans  CrossValerrors           Algorithm\n",
       "2       0.989623        0.031132        RandomForest\n",
       "3       0.989623        0.031132    GradientBoosting\n",
       "6       0.988375        0.029737             XGBoost\n",
       "0       0.988050        0.035849        DecisionTree\n",
       "7       0.987736        0.036792                LGBM\n",
       "1       0.986164        0.041509            AdaBoost\n",
       "4       0.983025        0.045771         KNeighboors\n",
       "5       0.886558        0.040542  LogisticRegression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_score_check('accuracy',X_res,y_res)  #높은데 표준편차가 큼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuarrcy\n",
    "#### 기본 : KNN, RF, LGBM,GB,XGB ,  샘플링 : RF,GB,XGB,DT,LGBM\n",
    "#### 공통 : RF,LGBM,GB,XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrossValMeans</th>\n",
       "      <th>CrossValerrors</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.873214</td>\n",
       "      <td>0.121494</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.790939</td>\n",
       "      <td>0.266704</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.776521</td>\n",
       "      <td>0.249579</td>\n",
       "      <td>GradientBoosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.693618</td>\n",
       "      <td>0.257619</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.597817</td>\n",
       "      <td>0.192862</td>\n",
       "      <td>AdaBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.542460</td>\n",
       "      <td>0.154511</td>\n",
       "      <td>DecisionTree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.540344</td>\n",
       "      <td>0.332790</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.511772</td>\n",
       "      <td>0.200888</td>\n",
       "      <td>KNeighboors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrossValMeans  CrossValerrors           Algorithm\n",
       "7       0.873214        0.121494                LGBM\n",
       "6       0.790939        0.266704             XGBoost\n",
       "3       0.776521        0.249579    GradientBoosting\n",
       "2       0.693618        0.257619        RandomForest\n",
       "1       0.597817        0.192862            AdaBoost\n",
       "0       0.542460        0.154511        DecisionTree\n",
       "5       0.540344        0.332790  LogisticRegression\n",
       "4       0.511772        0.200888         KNeighboors"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_score_check('roc_auc',X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CrossValMeans</th>\n",
       "      <th>CrossValerrors</th>\n",
       "      <th>Algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999592</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>GradientBoosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999371</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>RandomForest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.998916</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>LGBM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.998615</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>XGBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.988272</td>\n",
       "      <td>0.035185</td>\n",
       "      <td>DecisionTree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.986420</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>AdaBoost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.982779</td>\n",
       "      <td>0.050787</td>\n",
       "      <td>KNeighboors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.939449</td>\n",
       "      <td>0.051767</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CrossValMeans  CrossValerrors           Algorithm\n",
       "3       0.999592        0.001223    GradientBoosting\n",
       "2       0.999371        0.001887        RandomForest\n",
       "7       0.998916        0.003253                LGBM\n",
       "6       0.998615        0.004155             XGBoost\n",
       "0       0.988272        0.035185        DecisionTree\n",
       "1       0.986420        0.040741            AdaBoost\n",
       "4       0.982779        0.050787         KNeighboors\n",
       "5       0.939449        0.051767  LogisticRegression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_score_check('roc_auc',X_res,y_res)  # roc_auc에선 압도적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ROC-AUC\n",
    "#### 기본 : LGBM, XGB, GB  샘플링 : GB,RF,LGBM,XGB\n",
    "#### 공통 : LGBM,XGB,GB( RF가 샘플링에서 너무 좋으므로 RF차용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델간 유사도 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>GradientBoosting</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>LGBM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForest</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569542</td>\n",
       "      <td>0.615362</td>\n",
       "      <td>0.435320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoosting</th>\n",
       "      <td>0.569542</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819548</td>\n",
       "      <td>0.570468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.615362</td>\n",
       "      <td>0.819548</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.435320</td>\n",
       "      <td>0.570468</td>\n",
       "      <td>0.778971</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  RandomForest  GradientBoosting   XGBoost      LGBM\n",
       "RandomForest          1.000000          0.569542  0.615362  0.435320\n",
       "GradientBoosting      0.569542          1.000000  0.819548  0.570468\n",
       "XGBoost               0.615362          0.819548  1.000000  0.778971\n",
       "LGBM                  0.435320          0.570468  0.778971  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred=[]\n",
    "for cf in classifiers:\n",
    "    cf.fit(X_res,y_res)\n",
    "    pred.append(cf.predict(X_test))\n",
    "pred_df=pd.DataFrame(pred).T\n",
    "pred_df.columns=[\"DecisionTree\",\"AdaBoost\",\n",
    "\"RandomForest\",\"GradientBoosting\",\"KNeighboors\",\"LogisticRegression\",'XGBoost','LGBM']\n",
    "display(pred_df[['RandomForest','GradientBoosting','XGBoost','LGBM']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  상위 모델 간  상관관계 확인 , Boosting 류는 큰 꽤 높은 상관관계를 가지고 있다 ( 앙상블에 유의 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#튜닝\n",
    "import scipy as sp\n",
    "from scipy.stats import randint as sp_randint\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF=RandomForestClassifier()\n",
    "GB=GradientBoostingClassifier()\n",
    "XGB=XGBClassifier()\n",
    "LGB=LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[RF,GB,XGB,LGB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 648 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   10.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 13.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 16.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 22.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RF_param_grid = {'class_weight' :['balanced', 'balanced_subsample',None],\n",
    "                 'criterion' : ['gini','entropy'],\n",
    "                 'max_depth': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n",
    "                 'max_features':['log2',None,'auto'],\n",
    "                 'n_estimators' : [10, 30, 60, 100]}\n",
    "\n",
    "gsRF = GridSearchCV(RF,RF_param_grid, cv=kfold,verbose=2,n_jobs= -1)\n",
    "gsRF.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=64, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9949858978376684"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gsRF.best_estimator_)\n",
    "display(gsRF.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_stack=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=64, max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=64, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_stack.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feature=pd.Series(RF_stack.feature_importances_).sort_values(ascending=False).index.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6400 candidates, totalling 19200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 344 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 774 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1340 tasks      | elapsed:   34.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2000 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2545 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3214 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3851 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4939 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 6102 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 7359 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9051 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 10113 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=-1)]: Done 11351 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 13398 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 14653 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=-1)]: Done 17358 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=-1)]: Done 19045 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 19200 out of 19200 | elapsed: 13.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gb_param_grid = {'learning_rate' :[0.01, 0.05, 0.1, 0.5, 1],\n",
    "             'n_estimators' :[4, 8, 32, 64, 100],\n",
    "                 'min_samples_split' : [0.1,0.3,0.6,2],\n",
    "              'max_depth': [1,3,5,12],\n",
    "              'min_samples_leaf' :[0.1,0.3,0.5,1],\n",
    "              'max_features' : [20,40,60,None]\n",
    "              }\n",
    "\n",
    "gsGB = GridSearchCV(GB,gb_param_grid, cv=3,verbose=2,n_jobs= -1)\n",
    "gsGB.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.5, loss='deviance', max_depth=5,\n",
       "              max_features=20, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=0.6,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=64,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gsGB.best_estimator_)\n",
    "display(gsGB.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_stack=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "                       learning_rate=0.5, loss='deviance', max_depth=5,\n",
    "                       max_features=20, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=0.6,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=64,\n",
    "                       n_iter_no_change=None, presort='auto', random_state=None,\n",
    "                       subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "                       verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.5, loss='deviance', max_depth=5,\n",
       "              max_features=20, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=0.6,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=64,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB_stack.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_feature=pd.Series(GB_stack.feature_importances_).sort_values(ascending=False).index.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1080 candidates, totalling 10800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 20.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 29.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 39.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 51.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 64.1min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 75.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6829 tasks      | elapsed: 81.7min\n",
      "[Parallel(n_jobs=-1)]: Done 7922 tasks      | elapsed: 130.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9097 tasks      | elapsed: 144.8min\n",
      "[Parallel(n_jobs=-1)]: Done 10352 tasks      | elapsed: 162.1min\n",
      "[Parallel(n_jobs=-1)]: Done 10800 out of 10800 | elapsed: 168.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 48min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_param_grid = {'learning_rate' : [0.01,0.03,0.06,0.09],\n",
    "              'n_estimators' : [50,100,150],\n",
    "                  'booster' : ['dart','gbtree'],\n",
    "              'max_depth': [3,5,8,12,20],\n",
    "              'colsample_bytree' :[0.3,0.8,1],\n",
    "              'gamma': [0,1,5] \n",
    "              }\n",
    "\n",
    "gsXGB = GridSearchCV(XGB,xgb_param_grid, cv=kfold,verbose=2,n_jobs= -1)\n",
    "gsXGB.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.09,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=150, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9934189909119399"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gsXGB.best_estimator_)\n",
    "display(gsXGB.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_stack=XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
    "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.09,\n",
    "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
    "       n_estimators=150, n_jobs=1, nthread=None,\n",
    "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
    "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
    "       subsample=1, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.09,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=150, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_stack.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Feature importance is not defined for Booster type dart",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-4455a53462d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXGB_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m \u001b[1;31m# dart에선 불가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'booster'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'gbtree'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m             raise AttributeError('Feature importance is not defined for Booster type {}'\n\u001b[1;32m--> 541\u001b[1;33m                                  .format(self.booster))\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimportance_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Feature importance is not defined for Booster type dart"
     ]
    }
   ],
   "source": [
    "XGB_stack.feature_importances_ # dart에선 불가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB=LGBMClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10000 candidates, totalling 30000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8434 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9784 tasks      | elapsed: 12.1min\n",
      "[Parallel(n_jobs=-1)]: Done 11234 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 12784 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 14434 tasks      | elapsed: 17.6min\n",
      "[Parallel(n_jobs=-1)]: Done 16184 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done 18034 tasks      | elapsed: 22.0min\n",
      "[Parallel(n_jobs=-1)]: Done 19984 tasks      | elapsed: 24.3min\n",
      "[Parallel(n_jobs=-1)]: Done 22034 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=-1)]: Done 24184 tasks      | elapsed: 29.4min\n",
      "[Parallel(n_jobs=-1)]: Done 26434 tasks      | elapsed: 32.1min\n",
      "[Parallel(n_jobs=-1)]: Done 28784 tasks      | elapsed: 34.9min\n",
      "[Parallel(n_jobs=-1)]: Done 30000 out of 30000 | elapsed: 36.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score reached: 0.9974929489188342 with params: {'boosting_type': 'gbdt', 'colsample_bytree': 0.9788290104601244, 'min_child_samples': 131, 'min_child_weight': 0.01, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'subsample': 0.35982180753740106} \n",
      "Wall time: 36min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LGB_param_grid = {\"boosting_type\" : ['gbdt','dart'],\n",
    "                  'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "rsLGB = RandomizedSearchCV(LGB, param_distributions=LGB_param_grid, n_iter=10000, random_state=random_state,verbose=1,n_jobs=-1)\n",
    "rsLGB.fit(X_res, y_res)\n",
    "print('Best score reached: {} with params: {} '.format(rsLGB.best_score_, rsLGB.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9974929489188342"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.9788290104601244,\n",
       " 'min_child_samples': 131,\n",
       " 'min_child_weight': 0.01,\n",
       " 'reg_alpha': 0.1,\n",
       " 'reg_lambda': 0.1,\n",
       " 'subsample': 0.35982180753740106}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rsLGB.best_score_)\n",
    "display(rsLGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_stack=LGBMClassifier(\n",
    "    boosting_type = 'gbdt',\n",
    "    colsample_bytree=  0.9788290104601244,\n",
    "    min_child_samples= 131,\n",
    "    min_child_weight= 0.01,\n",
    "    reg_alpha= 0.1,\n",
    "    reg_lambda= 0.1,\n",
    "    subsample= 0.35982180753740106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=0.9788290104601244, importance_type='split',\n",
       "        learning_rate=0.1, max_depth=-1, min_child_samples=131,\n",
       "        min_child_weight=0.01, min_split_gain=0.0, n_estimators=100,\n",
       "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "        reg_alpha=0.1, reg_lambda=0.1, silent=True,\n",
       "        subsample=0.35982180753740106, subsample_for_bin=200000,\n",
       "        subsample_freq=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LGB_stack.fit(X_res,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_feature=pd.Series(LGB_stack.feature_importances_).sort_values(ascending=False).index.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분할 선택 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rank=pd.concat([pd.Series(rf_feature),pd.Series(gb_feature),pd.Series(lgb_feature)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_rank.columns=['RF','GB','LGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "important=feature_rank.iloc[:20,].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "important=pd.Series(important).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res2=X_res.iloc[:,important]\n",
    "X_cut_test=X_test.iloc[:,important]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RF_stack.fit(X_res2,y_res)\n",
    "#GB_stack.fit(X_res2,y_res)\n",
    "#LGB_stack.fit(X_res2,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF_stack.predict(X_cut_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_list=feature_rank['RF'][:40]\n",
    "gb_list=feature_rank['GB'][:20]\n",
    "lgb_list=feature_rank['LGB'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfdf=X_res.iloc[:,rf_list]\n",
    "rftest=X_test.iloc[:,rf_list]\n",
    "\n",
    "gbdf=X_res.iloc[:,gb_list]\n",
    "gbtest=X_test.iloc[:,gb_list]\n",
    "\n",
    "lgbdf=X_res.iloc[:,lgb_list]\n",
    "lgbtest=X_test.iloc[:,lgb_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "        colsample_bytree=0.9788290104601244, importance_type='split',\n",
       "        learning_rate=0.1, max_depth=-1, min_child_samples=131,\n",
       "        min_child_weight=0.01, min_split_gain=0.0, n_estimators=100,\n",
       "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "        reg_alpha=0.1, reg_lambda=0.1, silent=True,\n",
       "        subsample=0.35982180753740106, subsample_for_bin=200000,\n",
       "        subsample_freq=0)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_stack.fit(rfdf,y_res)\n",
    "GB_stack.fit(gbdf,y_res)\n",
    "LGB_stack.fit(lgbdf,y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.8, 0.5, 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0.7, 1. , 1. ,\n",
       "       1. , 1. , 1. , 1. , 1. , 0.4, 1. , 0.5, 1. , 0.8, 1. , 1. , 1. ,\n",
       "       1. , 0.5, 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ,\n",
       "       1. , 0. , 1. , 1. , 1. , 1. , 0.3, 1. , 1. , 0.9, 1. , 1. , 1. ,\n",
       "       0.2, 1. , 1. , 1. , 1. , 1. , 0.3, 1. , 1. , 1. , 1. , 1. , 1. ,\n",
       "       1. , 1. , 0.9, 1. , 1. , 1. , 0.9, 0.9, 1. , 1. , 1. , 1. , 1. ,\n",
       "       1. , 1. , 1. , 1. , 0.6, 0.5, 0.8, 1. , 1. , 1. , 0.7, 0.8, 1. ,\n",
       "       1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 0.7, 1. , 1. , 1. , 1. ,\n",
       "       0.6, 1. , 0.9, 1. , 1. , 1. , 1. , 1. , 0.7, 0.7, 1. , 1. , 1. ,\n",
       "       1. , 1. , 1. , 0.3, 1. , 0. , 0.5, 0.5, 1. , 0.6])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_stack.predict_proba(rftest)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(GB_stack.predict(gbdf),y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_prob=RF_stack.predict_proba(rftest)[:,1]\n",
    "GB_prob=GB_stack.predict_proba(gbtest)[:,1]\n",
    "LGB_prob=LGB_stack.predict_proba(lgbtest)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=RF_prob*0.2\n",
    "b=GB_prob*0.6\n",
    "c=LGB_prob*0.2\n",
    "d=a+b+c\n",
    "d[d<=0.8]=0\n",
    "d[d>=0.8]=1\n",
    "# PRED값, LOSS봐가면서 확인 이 값이 제출값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=GB_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_clf=[RF_stack,GB_stack,XGB_stack,LGB_stack]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_test2, y_train2,y_test2 = train_test_split(X_res,y_res, test_size=0.3, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=64, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_clf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9753086419753085"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8*(20/81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_GB=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.5, loss='deviance', max_depth=5,\n",
    "              max_features=2, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=0.6,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=64,\n",
    "              n_iter_no_change=None, presort='auto', random_state=None,\n",
    "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False)\n",
    "#Gradient Boosting이 loss 제일 적게 나와서 메타모델로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "             max_depth=64, max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.5, loss='deviance', max_depth=5,\n",
       "               max_features=20, max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=0.6,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=64,\n",
       "               n_iter_no_change=None, presort='auto', random_state=None,\n",
       "               subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "               verbose=0, warm_start=False),\n",
       " XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "        colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.09,\n",
       "        max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "        n_estimators=150, n_jobs=1, nthread=None,\n",
       "        objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "        reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "        subsample=1, verbosity=1),\n",
       " LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
       "         colsample_bytree=0.9788290104601244, importance_type='split',\n",
       "         learning_rate=0.1, max_depth=-1, min_child_samples=131,\n",
       "         min_child_weight=0.01, min_split_gain=0.0, n_estimators=100,\n",
       "         n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "         reg_alpha=0.1, reg_lambda=0.1, silent=True,\n",
       "         subsample=0.35982180753740106, subsample_for_bin=200000,\n",
       "         subsample_freq=0)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stack_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [log_loss]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [GradientBoostingClassifier]\n",
      "    fold  0:  [0.00251655]\n",
      "    fold  1:  [0.02408842]\n",
      "    fold  2:  [0.00466874]\n",
      "    fold  3:  [0.00443416]\n",
      "    fold  4:  [0.01440617]\n",
      "    fold  5:  [0.00503444]\n",
      "    fold  6:  [0.00285472]\n",
      "    fold  7:  [0.01682255]\n",
      "    fold  8:  [0.00522641]\n",
      "    fold  9:  [0.00230709]\n",
      "    ----\n",
      "    MEAN:     [0.00823593] + [0.00711623]\n",
      "    FULL:     [0.00823865]\n",
      "\n",
      "model  1:     [XGBClassifier]\n",
      "    fold  0:  [0.00186799]\n",
      "    fold  1:  [0.03830202]\n",
      "    fold  2:  [0.00253037]\n",
      "    fold  3:  [0.01299204]\n",
      "    fold  4:  [0.01447978]\n",
      "    fold  5:  [0.00381442]\n",
      "    fold  6:  [0.00281507]\n",
      "    fold  7:  [0.03729709]\n",
      "    fold  8:  [0.01475897]\n",
      "    fold  9:  [0.00331852]\n",
      "    ----\n",
      "    MEAN:     [0.01321763] + [0.01323149]\n",
      "    FULL:     [0.01320821]\n",
      "\n",
      "model  2:     [LGBMClassifier]\n",
      "    fold  0:  [0.00340010]\n",
      "    fold  1:  [0.01656969]\n",
      "    fold  2:  [0.00743379]\n",
      "    fold  3:  [0.01442855]\n",
      "    fold  4:  [0.01319768]\n",
      "    fold  5:  [0.01012760]\n",
      "    fold  6:  [0.00408016]\n",
      "    fold  7:  [0.01889792]\n",
      "    fold  8:  [0.01284066]\n",
      "    fold  9:  [0.00365792]\n",
      "    ----\n",
      "    MEAN:     [0.01046341] + [0.00532466]\n",
      "    FULL:     [0.01046268]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9979123173277662"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from vecstack import stacking\n",
    "S_train, S_test = stacking(stack_clf[1:],                     # list of models\n",
    "                           X_train2, y_train2, X_test2,   # data\n",
    "                           regression=False,           # classification task (if you need \n",
    "                                                       #     regression - set to True)\n",
    "                           needs_proba=True,          # predict class labels (if you need \n",
    "                                                       #     probabilities - set to True) \n",
    "                           metric=None,      # metric: callable\n",
    "                           n_folds=10,                  # number of folds\n",
    "                           stratified=True,            # stratified split for folds\n",
    "                           shuffle=True,               # shuffle the data\n",
    "                           random_state=random_state,             # ensure reproducibility\n",
    "                           verbose=2)                  # print all info\n",
    "\n",
    "meta_model = meta_GB.fit(S_train, y_train2)\n",
    "display(accuracy_score(y_test2, meta_GB.predict(S_test)))\n",
    "#Gradient Boosting이 loss 제일 적게 나와서 메타모델로 사용\n",
    "# + LGBM이 LOSS가 크고 변동도 심해서 제외 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [log_loss]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [GradientBoostingClassifier]\n",
      "    fold  0:  [0.00307698]\n",
      "    fold  1:  [0.01502805]\n",
      "    fold  2:  [0.00331364]\n",
      "    fold  3:  [0.00713222]\n",
      "    fold  4:  [0.00726791]\n",
      "    fold  5:  [0.00520278]\n",
      "    fold  6:  [0.00346725]\n",
      "    fold  7:  [0.02699599]\n",
      "    fold  8:  [0.00349445]\n",
      "    fold  9:  [0.00314892]\n",
      "    ----\n",
      "    MEAN:     [0.00781282] + [0.00727723]\n",
      "    FULL:     [0.00780563]\n",
      "\n",
      "model  1:     [XGBClassifier]\n",
      "    fold  0:  [0.00186799]\n",
      "    fold  1:  [0.03830202]\n",
      "    fold  2:  [0.00253037]\n",
      "    fold  3:  [0.01299204]\n",
      "    fold  4:  [0.01447978]\n",
      "    fold  5:  [0.00381442]\n",
      "    fold  6:  [0.00281507]\n",
      "    fold  7:  [0.03729709]\n",
      "    fold  8:  [0.01475897]\n",
      "    fold  9:  [0.00331852]\n",
      "    ----\n",
      "    MEAN:     [0.01321763] + [0.01323149]\n",
      "    FULL:     [0.01320821]\n",
      "\n",
      "model  2:     [LGBMClassifier]\n",
      "    fold  0:  [0.00340010]\n",
      "    fold  1:  [0.01656969]\n",
      "    fold  2:  [0.00743379]\n",
      "    fold  3:  [0.01442855]\n",
      "    fold  4:  [0.01319768]\n",
      "    fold  5:  [0.01012760]\n",
      "    fold  6:  [0.00408016]\n",
      "    fold  7:  [0.01889792]\n",
      "    fold  8:  [0.01284066]\n",
      "    fold  9:  [0.00365792]\n",
      "    ----\n",
      "    MEAN:     [0.01046341] + [0.00532466]\n",
      "    FULL:     [0.01046268]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9968684759916493"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from vecstack import stacking\n",
    "S_train, S_test = stacking(stack_clf[1:],                     # list of models\n",
    "                           X_train2, y_train2, X_test2,   # data\n",
    "                           regression=False,           # classification task (if you need \n",
    "                                                       #     regression - set to True)\n",
    "                           needs_proba=True,          # predict class labels (if you need \n",
    "                                                       #     probabilities - set to True) \n",
    "                           metric=None,      # metric: callable\n",
    "                           n_folds=10,                  # number of folds\n",
    "                           stratified=True,            # stratified split for folds\n",
    "                           shuffle=True,               # shuffle the data\n",
    "                           random_state=random_state,             # ensure reproducibility\n",
    "                           verbose=2)                  # print all info\n",
    "\n",
    "meta_model = meta_GB.fit(S_train, y_train2)\n",
    "display(accuracy_score(y_test2, meta_GB.predict(S_test)))\n",
    "#Gradient Boosting이 loss 제일 적게 나와서 메타모델로 사용\n",
    "# + LGBM이 LOSS가 크고 변동도 심해서 제외 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [log_loss]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [2]\n",
      "\n",
      "model  0:     [GradientBoostingClassifier]\n",
      "    fold  0:  [0.00515270]\n",
      "    fold  1:  [0.01483063]\n",
      "    fold  2:  [0.00371005]\n",
      "    fold  3:  [0.00308346]\n",
      "    fold  4:  [0.00258384]\n",
      "    fold  5:  [0.00259799]\n",
      "    fold  6:  [0.00483948]\n",
      "    fold  7:  [0.00560203]\n",
      "    fold  8:  [0.00447358]\n",
      "    fold  9:  [0.00373667]\n",
      "    ----\n",
      "    MEAN:     [0.00506104] + [0.00340343]\n",
      "    FULL:     [0.00506281]\n",
      "\n",
      "model  1:     [XGBClassifier]\n",
      "    fold  0:  [0.00489372]\n",
      "    fold  1:  [0.00299520]\n",
      "    fold  2:  [0.00265667]\n",
      "    fold  3:  [0.00229540]\n",
      "    fold  4:  [0.00184527]\n",
      "    fold  5:  [0.00222083]\n",
      "    fold  6:  [0.00339441]\n",
      "    fold  7:  [0.00368340]\n",
      "    fold  8:  [0.00269404]\n",
      "    fold  9:  [0.00458762]\n",
      "    ----\n",
      "    MEAN:     [0.00312666] + [0.00096007]\n",
      "    FULL:     [0.00312578]\n",
      "\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from vecstack import stacking\n",
    "S_train, S_test = stacking([GB_stack,XGB_stack],                     # list of models\n",
    "                           gbdf, y_res, gbtest,   # data\n",
    "                           regression=False,           # classification task (if you need \n",
    "                                                       #     regression - set to True)\n",
    "                           needs_proba=True,          # predict class labels (if you need \n",
    "                                                       #     probabilities - set to True) \n",
    "                           metric=None,      # metric: callable\n",
    "                           n_folds=10,                  # number of folds\n",
    "                           stratified=True,            # stratified split for folds\n",
    "                           shuffle=True,               # shuffle the data\n",
    "                           random_state=random_state,             # ensure reproducibility\n",
    "                           verbose=2)                  # print all info\n",
    "\n",
    "meta_model = meta_GB.fit(S_train, y_res)\n",
    "#Gradient Boosting이 loss 제일 적게 나와서 메타모델로 사용\n",
    "# + LGBM이 LOSS가 크고 변동도 심해서 제외 시도\n",
    "# + 실제 값 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=meta_model.predict(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=meta_model.predict_proba(S_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred<0.7]=0\n",
    "pred[pred>=0.7]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24,  48, 123, 165, 254, 258, 341, 413, 424, 425, 429], dtype=int64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_id[pd.Series(pred)[pd.Series(pred)==0].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=pd.concat([pd.Series(test_id),pd.Series(pred)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.columns=['inst_id','OC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.to_csv('과제제출1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACKING류는 90점대를 넘지 못했고\n",
    "# GB모델을 가중치 줘가면서 Voting한게 가장 성능이 좋았다 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
